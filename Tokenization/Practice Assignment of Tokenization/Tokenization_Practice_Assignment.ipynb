{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization_Practice_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdsqd9dAcNXO",
        "colab_type": "text"
      },
      "source": [
        "# **Tokenization Practice Assignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVhdKlIMcUB5",
        "colab_type": "text"
      },
      "source": [
        "Q1. Use the given article to tokenize, with every type of tokenization method used in learn tokenization. Please use that 3 step easy method to implement the Tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRFBGi9lcvoV",
        "colab_type": "text"
      },
      "source": [
        "**Article:** \n",
        "One of the joys of running the Whole Life Challenge is that I get a lot of my friends and family members to play along with me. It gives me an opportunity to talk about the Challenge with other players where the rubber meets the road, where people try and fit their life into the constraints of this online game.\n",
        "\n",
        "Recently, I had a great exchange with an old friend of my sister’s, Kate, and realized it was an exchange worth sharing. For Kate, her literal road bump was about traveling during the Challenge, but my advice applies to anyone who finds themselves asking, “How the heck do I make this work in my life?”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcMjTlLsdEdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 1:\n",
        "#Importing library here...\n",
        "\n",
        "\n",
        "\n",
        "#STEP 2:\n",
        "# Use Article blog here...\n",
        "\n",
        "\n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer here...\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}