{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcYcCR3RW1SZ",
        "colab_type": "text"
      },
      "source": [
        "# **Tokenization using Python NLTK**    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2qF3EiUYa7T",
        "colab_type": "text"
      },
      "source": [
        "![wakeupcoder](https://media.geeksforgeeks.org/wp-content/uploads/tokenizer.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxkHpsK_Yxsh",
        "colab_type": "text"
      },
      "source": [
        "# **How sent_tokenize works ?**\n",
        "\n",
        "The **sent_tokenize** function uses an instance of  **PunktSentenceTokenizer** from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and begining of sentence at what characters and punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cmmLrk3ZeHf",
        "colab_type": "text"
      },
      "source": [
        "# **Code #1: Sentence Tokenization – Splitting sentences in the paragraph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RUOT7k6XKW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentence tokenization helps us to split the screen on sentence basis \n",
        "#STEP 1:\n",
        "#Importing library \n",
        "from nltk.tokenize import sent_tokenize \n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data    \n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "sent_tokenize(text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO0D-9n7Ziio",
        "colab_type": "text"
      },
      "source": [
        "# **Code #2: PunktSentenceTokenizer – When we have huge chunks of data then it is efficient to use it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOx-bXV4ZyNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 1:\n",
        "#Importing library \n",
        "import nltk.data \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data  \n",
        "# Loading PunktSentenceTokenizer using English pickle file \n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') \n",
        "  \n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "tokenizer.tokenize(text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vre3zhVVaEra",
        "colab_type": "text"
      },
      "source": [
        "# **Code #3: Tokenize sentence of different language – One can also tokenize sentence from different languages using different pickle file other than English.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHc1M80aWbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 1:\n",
        "#Importing library \n",
        "import nltk.data \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data    \n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle') \n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "text = 'Hola amigo. Estoy bien.'\n",
        "spanish_tokenizer.tokenize(text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLJiXjPgalV0",
        "colab_type": "text"
      },
      "source": [
        "# **Code #4: Word Tokenization – Splitting words in a sentence.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhk3aDOQa7su",
        "colab_type": "text"
      },
      "source": [
        "How word_tokenize works?\n",
        "word_tokenize() function is a wrapper function that calls tokenize() on an instance of the TreebankWordTokenizer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pNbpiTTaoNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data   \n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "word_tokenize(text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDCBFPOfbQqp",
        "colab_type": "text"
      },
      "source": [
        "# **Code #5: Using TreebankWordTokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2K0wO3QbVyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# NOTE : These tokenizers work by separating the words using punctuation and spaces.\n",
        "# And as mentioned in the code outputs below it does not discard the punctuation, \n",
        "# allowing a user to decide what to do with the punctuations at the time of pre-processing.\n",
        "\n",
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import TreebankWordTokenizer \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data   \n",
        "tokenizer = TreebankWordTokenizer() \n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "tokenizer.tokenize(text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI1jYqtTb0m9",
        "colab_type": "text"
      },
      "source": [
        "# **Code #6: PunktWordTokenizer – It doen’t seperates the punctuation from the words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-IBLaMbb3zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import PunktWordTokenizer \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data     \n",
        "tokenizer = PunktWordTokenizer() \n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of  tokenizer\n",
        "tokenizer.tokenize(\"Let's see how it's working.\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j-FDXb8cONJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #WordPunctTokenizer – It seperates the punctuation from the words.\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer \n",
        "tokenizer = WordPunctTokenizer() \n",
        "tokenizer.tokenize(\"Let's see how it's working.\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHCNHeGxcac9",
        "colab_type": "text"
      },
      "source": [
        "# **Code #7: Using Regular Expression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFV9kOA_chGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 1:\n",
        "#Importing library\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "\n",
        "#STEP 2:\n",
        "# Creating Text data  \n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
        "text = \"Let's see how it's working.\"\n",
        "\n",
        "#STEP 3:\n",
        "# Implementation of tokenizer\n",
        "tokenizer.tokenize(text) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}